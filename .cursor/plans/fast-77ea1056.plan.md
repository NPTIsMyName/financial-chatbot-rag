<!-- 77ea1056-9254-4d91-b382-23b67f5b02cd 9af1197e-e48b-43f9-9420-64d371f62753 -->
# FastAPI demo for Gemini + Chroma chatbot

## Scope

- Keep current model stack: `ChatGoogleGenerativeAI(gemini-2.0-flash)` and `Chroma` with `HuggingFaceEndpointEmbeddings('BAAI/bge-m3')`.
- Add a FastAPI app exposing a `POST /api/chat` endpoint with simple session-based memory.
- Serve a minimal static HTML/JS page at `/` to demo chatting via fetch.

## Key design

- App startup:
  - Load `.env` (`GOOGLE_API_KEY`, optional `HF_TOKEN` if needed).
  - Initialize embeddings and `Chroma(persist_directory='chroma_store', embedding_function=embeddings)` once.
  - Build a factory to create a `ConversationalRetrievalChain` per session with `ConversationBufferMemory`.
- Session handling:
  - Client provides `session_id` (UUID) in each request. Server keeps an in-memory map: `session_id -> chain` (memory lives in that chain).
  - Optional TTL eviction is out-of-scope for now.
- Endpoint:
  - `POST /api/chat` accepts `{ session_id, message }`.
  - Creates or reuses the chain, calls with `{ "question": message }`, returns `{ answer }`.
- Frontend:
  - Simple page with input box, message list, and JS `fetch` to `/api/chat`.
  - Generate a stable `session_id` in `localStorage`.
- CORS: enable `fastapi.middleware.cors` for local testing.

## Files to add

- `server.py`: FastAPI app with startup init, `/api/chat`, and static route for demo page.
- `static/index.html`: Minimal chat UI using fetch.
- `requirements.txt`: Include FastAPI, Uvicorn, LangChain, Chroma, Google GenAI, HuggingFace, python-dotenv, etc. (pin to compatible versions).
- Optionally `README.md`: quick run steps.

## Essential snippets (non-exhaustive)

- Chain factory (adapted from `app_chroma.py`):
```python
# server.py (excerpt)
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate

def create_chain(vectorstore):
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash", temperature=0.1, google_api_key=os.getenv("GOOGLE_API_KEY"), max_output_tokens=512
    )
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    prompt = PromptTemplate(
        template=(
            """
            Bạn là trợ lý AI tổng hợp tin tức tài chính.\nTrả lời rõ ràng, tự nhiên bằng tiếng Việt.\nNếu không tìm thấy thông tin phù hợp, hãy nói:\n\"Xin lỗi, tôi không tìm thấy thông tin phù hợp trong dữ liệu hiện có.\"\n\nCâu hỏi: {question}\nNgữ cảnh: {context}\nTrả lời:
            """
        ),
        input_variables=["question", "context"],
    )
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
    )
```

- Endpoint contract:
```http
POST /api/chat
Content-Type: application/json
{
  "session_id": "string",
  "message": "string"
}
-> { "answer": "string" }
```


## Run

- `uvicorn server:app --reload --port 8000`
- Open `http://localhost:8000/`

## Risks/Notes

- In-memory sessions reset when server restarts; for persistence, later move memory to Redis or DB.
- Ensure `chroma_store/` path matches your existing vector DB.
- Verify environment variables loaded before app startup.

### To-dos

- [ ] Create FastAPI app `server.py` with startup init and CORS
- [ ] Load embeddings and Chroma from `chroma_store` at startup
- [ ] Implement chain factory reusing Gemini prompt and settings
- [ ] Add POST `/api/chat` with session-based memory map
- [ ] Serve `static/index.html` at `/` with minimal chat UI
- [ ] Add `requirements.txt` and run instructions for uvicorn